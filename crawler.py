import re
from playwright.sync_api import sync_playwright, TimeoutError
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from playwright_stealth import stealth_sync

# --- DB ì„¤ì • ---
DB_USER = 'root'
DB_PASSWORD = '1234'  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ìˆ˜ì •
DB_HOST = 'localhost'
DB_PORT = '3306'
DB_NAME = 'danawa'
# -----------------

# --- í¬ë¡¤ë§ ì„¤ì • ---
CATEGORIES = {
    'CPU': 'cpu',
    'RAM': 'RAM',
    # 'ë©”ì¸ë³´ë“œ': 'mainboard', # ìƒˆë¡œìš´ íŒŒì‹± í•¨ìˆ˜ ì¶”ê°€ í•„ìš”
}

# --- SQLAlchemy ì—”ì§„ ìƒì„± ---
try:
    engine = create_engine(f'mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')
    with engine.connect() as conn:
        print("âœ… DB ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"â— DB ì—°ê²° ì‹¤íŒ¨: {e}")
    exit()


def parse_cpu_specs(spec_string):
    """[ì‹ ì„¤] CPU ìŠ¤í™ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    specs = {}
    spec_parts = [part.strip() for part in spec_string.split('/')]

    for part in spec_parts:
        if 'ì†Œì¼“' in part:
            # ex: "ì¸í…”(ì†Œì¼“1851)" -> "ì†Œì¼“1851"
            socket_info = re.search(r'ì†Œì¼“\d+', part)
            if socket_info:
                specs['socket'] = socket_info.group()
        elif 'ì½”ì–´' in part:
            specs['cores'] = part
        elif 'ìŠ¤ë ˆë“œ' in part:
            specs['threads'] = part
        elif 'í´ëŸ­' in part:
            if 'ìµœëŒ€' in part:
                specs['clock_speed'] = part # ìµœëŒ€ í´ëŸ­ì„ ìš°ì„  ì €ì¥
            elif 'ê¸°ë³¸' in part and 'clock_speed' not in specs:
                specs['clock_speed'] = part # ìµœëŒ€ í´ëŸ­ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í´ëŸ­ ì €ì¥
        elif 'L3 ìºì‹œ' in part:
            specs['l3_cache'] = part
        elif 'L2 ìºì‹œ' in part:
            specs['l2_cache'] = part
        elif 'ë‚´ì¥ê·¸ë˜í”½' in part:
            specs['integrated_graphics'] = "ìˆìŒ" if 'íƒ‘ì¬' in part else "ì—†ìŒ"
    return specs


def scrape_category(page, category_name, query):
    """[ìˆ˜ì •] ëª©ë¡ í˜ì´ì§€ë§Œ ìŠ¤í¬ë˜í•‘í•˜ë„ë¡ ì¬êµ¬ì„±ëœ í•¨ìˆ˜"""
    print(f"\n--- '{category_name}' ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
    
    sql = text("""
        INSERT INTO parts (
            name, category, price, link, img_src, 
            socket, cores, threads, clock_speed, l2_cache, l3_cache, integrated_graphics
        ) VALUES (
            :name, :category, :price, :link, :img_src, 
            :socket, :cores, :threads, :clock_speed, :l2_cache, :l3_cache, :integrated_graphics
        )
        ON DUPLICATE KEY UPDATE
            price=VALUES(price), img_src=VALUES(img_src), socket=VALUES(socket), cores=VALUES(cores),
            threads=VALUES(threads), clock_speed=VALUES(clock_speed), l2_cache=VALUES(l2_cache), 
            l3_cache=VALUES(l3_cache), integrated_graphics=VALUES(integrated_graphics);
    """)

    with engine.connect() as conn:
        for page_num in range(1, 6): # 5í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
            url = f'https://search.danawa.com/dsearch.php?query={query}&page={page_num}'
            print(f"--- '{category_name}' ì¹´í…Œê³ ë¦¬, {page_num}í˜ì´ì§€ ëª©ë¡ ìˆ˜ì§‘ ---")
            
            try:
                page.goto(url, wait_until='load', timeout=15000)
                page.wait_for_selector('ul.product_list', timeout=10000)
                
                list_html = page.content()
                list_soup = BeautifulSoup(list_html, 'html.parser')
                product_items = list_soup.select('li.prod_item[id^="productItem"]')

                if not product_items:
                    print("--- í˜„ì¬ í˜ì´ì§€ì— ìƒí’ˆì´ ì—†ì–´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ---")
                    break
                
                for item in product_items:
                    # ... (ì´ì „ê³¼ ë™ì¼í•œ íŒŒì‹± ë¡œì§) ...
                    name_tag = item.select_one('p.prod_name > a')
                    price_tag = item.select_one('p.price_sect > a > strong')
                    img_tag = item.select_one('div.thumb_image img')

                    if not all([name_tag, price_tag, img_tag]):
                        continue

                    name = name_tag.text.strip()
                    link = name_tag['href']
                    img_src = "https:" + img_tag.get('data-original-src', img_tag['src'])

                    try:
                        price_str = price_tag.text.strip()
                        price = int(price_str.replace(',', ''))
                    except ValueError:
                        print(f"  - ê°€ê²© ì •ë³´ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤: {name} (ê°’: '{price_str}')")
                        continue
                    
                    spec_tag = item.select_one('div.spec_list')
                    spec_string = spec_tag.text.strip() if spec_tag else ""
                    
                    detailed_specs = {}
                    if category_name == 'CPU':
                        detailed_specs = parse_cpu_specs(spec_string)

                    params = {
                        "name": name, "category": category_name, "price": price, 
                        "link": link, "img_src": img_src,
                        "socket": detailed_specs.get('socket'),
                        "cores": detailed_specs.get('cores'),
                        "threads": detailed_specs.get('threads'),
                        "clock_speed": detailed_specs.get('clock_speed'),
                        "l2_cache": detailed_specs.get('l2_cache'),
                        "l3_cache": detailed_specs.get('l3_cache'),
                        "integrated_graphics": detailed_specs.get('integrated_graphics')
                    }
                    
                    conn.execute(sql, params)
                    print(f"  âœ… {name}")
                
                conn.commit()

            except Exception as e:
                # [ìˆ˜ì •] ìì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸ì„ ìœ„í•´ eë¥¼ ì§ì ‘ ì¶œë ¥
                print(f"--- {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ---")
                conn.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜„ì¬ íŠ¸ëœì­ì…˜ ë¡¤ë°±
                continue

def run_crawler():
    """ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        stealth_sync(page)
        
        page.set_extra_http_headers({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"})

        for category_name, query in CATEGORIES.items():
            scrape_category(page, category_name, query)
            
        browser.close()
        print("\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    run_crawler()