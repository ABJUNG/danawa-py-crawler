from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text

# --- DB ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼) ---
DB_USER = 'root'
DB_PASSWORD = '1234' # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ìˆ˜ì •
DB_HOST = 'localhost'
DB_PORT = '3306'
DB_NAME = 'danawa'
# --------------------------------

engine = create_engine(f'mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

CATEGORIES = {
    'CPU': 'cpu',
    'ì¿¨ëŸ¬': 'ì¿¨ëŸ¬',
    'ë©”ì¸ë³´ë“œ': 'mainboard',
    'RAM': 'RAM',
    'ê·¸ë˜í”½ì¹´ë“œ': 'vga',
    'SSD': 'ssd',
    'HDD': 'hdd',
    'íŒŒì›Œ': 'power',
    'ì¼€ì´ìŠ¤': 'pc case'
}

def scrape_category(page, category_name, query):
    """ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ ìŠ¤í¬ë©í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n--- '{category_name}' ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
    
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    # [ìˆ˜ì •ëœ ë¶€ë¶„] 1í˜ì´ì§€ë¶€í„° 5í˜ì´ì§€ê¹Œì§€ ìˆœíšŒí•©ë‹ˆë‹¤. (í•œ í˜ì´ì§€ë‹¹ ì•½ 40ê°œ * 5 = ì•½ 200ê°œ)
    for page_num in range(1, 6):
        url = f'https://search.danawa.com/dsearch.php?query={query}&page={page_num}'
        print(f"--- '{category_name}' ì¹´í…Œê³ ë¦¬, {page_num}í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ---")
        
        try:
            page.goto(url)
            page.wait_for_selector('ul.product_list', timeout=10000) # íƒ€ì„ì•„ì›ƒì„ 10ì´ˆë¡œ ì¤„ì„

            last_height = page.evaluate("document.body.scrollHeight")
            while True:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(1500)
                new_height = page.evaluate("document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')
            product_list = soup.select('li.prod_item[id^="productItem"]')

            # ë§Œì•½ í˜„ì¬ í˜ì´ì§€ì— ìƒí’ˆì´ ì—†ë‹¤ë©´, í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•˜ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
            if not product_list:
                print("--- í˜„ì¬ í˜ì´ì§€ì— ìƒí’ˆì´ ì—†ì–´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ---")
                break

            for item in product_list:
                try:
                    name = item.select_one('p.prod_name > a').text.strip()
                    price_str = item.select_one('p.price_sect > a > strong').text.strip()
                    price = int(price_str.replace(',', ''))
                    link = item.select_one('p.prod_name > a')['href']
                    img_src = "https:" + item.select_one('div.thumb_image img')['src']

                    with engine.connect() as conn:
                        sql = text("""
                            INSERT INTO parts (name, category, price, link, img_src)
                            VALUES (:name, :category, :price, :link, :img_src)
                            ON DUPLICATE KEY UPDATE
                                name = VALUES(name), category = VALUES(category), price = VALUES(price), img_src = VALUES(img_src);
                        """)
                        conn.execute(sql, {
                            "name": name, "category": category_name, "price": price, "link": link, "img_src": img_src
                        })
                        conn.commit()

                except (AttributeError, ValueError, TypeError):
                    continue
        
        except Exception as e:
            print(f"--- {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
            continue
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

# ... (run_crawlerì™€ if __name__ == "__main__": ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
def run_crawler():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        for category_name, query in CATEGORIES.items():
            scrape_category(page, category_name, query)

        browser.close()
        print("\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    run_crawler()