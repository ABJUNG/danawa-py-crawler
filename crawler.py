import re
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text

# --- DB ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼) ---
DB_USER = 'root'
DB_PASSWORD = '1234' # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ ìˆ˜ì •
DB_HOST = 'localhost'
DB_PORT = '3306'
DB_NAME = 'danawa'
# --------------------------------

engine = create_engine(f'mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')

CATEGORIES = {
    'CPU': 'cpu',
    'ì¿¨ëŸ¬': 'ì¿¨ëŸ¬',
    'ë©”ì¸ë³´ë“œ': 'mainboard',
    'RAM': 'RAM',
    'ê·¸ë˜í”½ì¹´ë“œ': 'vga',
    'SSD': 'ssd',
    'HDD': 'hdd',
    'íŒŒì›Œ': 'power',
    'ì¼€ì´ìŠ¤': 'pc case'
}

def parse_specs(name, category):
    specs = {'socket': None, 'core_type': None, 'ram_capacity': None, 'chipset': None}
    
    if category == 'CPU' or category == 'ë©”ì¸ë³´ë“œ':
        match = re.search(r'\((LGA[0-9]+|AM[45])\)', name)
        if match: specs['socket'] = match.group(1)

    if category == 'CPU':
        if 'ì½”ì–´i9' in name or 'ë¼ì´ì   9' in name: specs['core_type'] = 'i9/ë¼ì´ì  9'
        elif 'ì½”ì–´i7' in name or 'ë¼ì´ì   7' in name: specs['core_type'] = 'i7/ë¼ì´ì  7'
        elif 'ì½”ì–´i5' in name or 'ë¼ì´ì   5' in name: specs['core_type'] = 'i5/ë¼ì´ì  5'
        elif 'ì½”ì–´i3' in name or 'ë¼ì´ì   3' in name: specs['core_type'] = 'i3/ë¼ì´ì  3'
    
    if category == 'RAM':
        match = re.search(r'(\d+GB)', name)
        if match: specs['ram_capacity'] = match.group(1)

    if category == 'ê·¸ë˜í”½ì¹´ë“œ':
        if 'GeForce' in name: specs['chipset'] = 'NVIDIA'
        elif 'Radeon' in name: specs['chipset'] = 'AMD'
        
    return specs

def scrape_category(page, category_name, query):
    """ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆ ì •ë³´ë¥¼ ìŠ¤í¬ë©í•˜ëŠ” í•¨ìˆ˜ (ë„¤íŠ¸ì›Œí¬ ì•ˆì • ìƒíƒœ ëŒ€ê¸° ì ìš©)"""
    url = f'https://search.danawa.com/dsearch.php?query={query}'
    print(f"\n--- '{category_name}' ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
    
    try:
        page.goto(url)
        page.wait_for_selector('ul.product_list', timeout=30000)

        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # [ìˆ˜ì •ëœ ë¶€ë¶„] ë” ì•ˆì •ì ì¸ ìŠ¤í¬ë¡¤ ë° ëŒ€ê¸° ë¡œì§
        last_height = page.evaluate("document.body.scrollHeight")
        while True:
            # í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            
            # ë„¤íŠ¸ì›Œí¬ í™œë™ì´ ì ì í•´ì§ˆ ë•Œê¹Œì§€ ìµœëŒ€ 5ì´ˆê°„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
            # ì´ê²ƒì´ ê³ ì • ì‹œê°„ ëŒ€ê¸°ë³´ë‹¤ í›¨ì”¬ ì•ˆì •ì ì…ë‹ˆë‹¤.
            page.wait_for_load_state('networkidle', timeout=5000)
            
            new_height = page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                break # ë†’ì´ ë³€í™”ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì½˜í…ì¸ ê°€ ë¡œë“œëœ ê²ƒì´ë¯€ë¡œ ë°˜ë³µ ì¢…ë£Œ
            last_height = new_height
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

        html = page.content()
        soup = BeautifulSoup(html, 'html.parser')
        product_list = soup.select('li.prod_item[id^="productItem"]')
        print(f"--- ìµœì¢… {len(product_list)}ê°œì˜ '{category_name}' ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ---")

        for item in product_list:
            try:
                # ... (ì´í•˜ DB ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
                name = item.select_one('p.prod_name > a').text.strip()
                price_str = item.select_one('p.price_sect > a > strong').text.strip()
                price = int(price_str.replace(',', ''))
                link = item.select_one('p.prod_name > a')['href']
                img_src = "https:" + item.select_one('div.thumb_image img')['src']
                specs = parse_specs(name, category_name)

                with engine.connect() as conn:
                # SQL ì¿¼ë¦¬ì— ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤ ì¶”ê°€
                    sql = text("""
                        INSERT INTO parts (name, category, price, link, img_src, socket, core_type, ram_capacity, chipset)
                        VALUES (:name, :category, :price, :link, :img_src, :socket, :core_type, :ram_capacity, :chipset)
                        ON DUPLICATE KEY UPDATE
                            name=VALUES(name), category=VALUES(category), price=VALUES(price), img_src=VALUES(img_src),
                            socket=VALUES(socket), core_type=VALUES(core_type), ram_capacity=VALUES(ram_capacity), chipset=VALUES(chipset);
                    """)
                    conn.execute(sql, {
                        "name": name, "category": category_name, "price": price, "link": link, "img_src": img_src,
                        "socket": specs['socket'], "core_type": specs['core_type'], 
                        "ram_capacity": specs['ram_capacity'], "chipset": specs['chipset']
                    })
                    conn.commit()

            except (AttributeError, ValueError, TypeError):
                continue
    
    except Exception as e:
        print(f"--- '{category_name}' ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")

def run_crawler():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        for category_name, query in CATEGORIES.items():
            scrape_category(page, category_name, query)

        browser.close()
        print("\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    run_crawler()